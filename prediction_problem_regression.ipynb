{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b2d03caf",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report - Regression\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd3fca",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis\n",
    "\n",
    "- First, there were columns that had characters in them (%, \\$) so I filtered cleaned the columns to be numeric. \n",
    "- There were different variations to quantify the number of listings a host has, so using correlations and some trial and error only one of the host listing count predictors was selected. \n",
    "- Review scores are averaged because they are very highly correlated.\n",
    "- There was an extremely high outlier for price that was extremely skewing the model so I filter out the top  and bottom 0.7\\% of price observations.\n",
    "- Similairly there was a large outlier in minimum nights that was skewing it's significance, so the top 0.1\\% of minimum nights observations were dropped\n",
    "- There were some columns that were dates so they were converted to datetime objects and also a column was created for 'months since' that date to make the date columns easier to utilize\n",
    "- Property types and Neighbourhoods had some observations with low occurances so those were grouped into 'Other'\n",
    "- host_is_superhost, acceptance_rate, and resposne_rate were imputed using simple models\n",
    "    - the remaining missing values were imputed naively with the columns median value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53377f61",
   "metadata": {},
   "source": [
    "## 2) Data Cleaning/Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f042038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "055d7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('datasets/train_regression.csv')\n",
    "raw_test = pd.read_csv('datasets/test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe14fa6",
   "metadata": {},
   "source": [
    "## Clean and Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6255b",
   "metadata": {},
   "source": [
    "### General Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a21065c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the raw datasets\n",
    "train = raw_train.copy()\n",
    "test = raw_test.copy()\n",
    "\n",
    "# Clean 'price' column: remove '$' and ',' characters, and convert to float\n",
    "train['price'] = train['price'].str.replace(',', '').str.replace('$', '', regex=False).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c53ec301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'host_acceptance_rate' and 'host_response_rate' columns to float and scale by dividing by 100\n",
    "train['acceptance_rate'] = train['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "train['response_rate'] = train['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "test['acceptance_rate'] = test['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "test['response_rate'] = test['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "test.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Extract numeric values from 'bathrooms_text' column and convert to float\n",
    "train['bathrooms_num'] = train['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "test['bathrooms_num'] = test['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Fill missing values in 'bathrooms_num' where 'Half-bath' is mentioned in 'bathrooms_text' with 0.5\n",
    "train.loc[train['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & train['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n",
    "test.loc[test['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & test['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ace60bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0000\n",
       "1       0.0000\n",
       "2       4.8200\n",
       "3       4.8350\n",
       "4       4.7525\n",
       "         ...  \n",
       "3333    4.6875\n",
       "3334    4.7100\n",
       "3335    4.6350\n",
       "3336    4.8125\n",
       "3337    4.7425\n",
       "Name: review_scores_avg, Length: 3338, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date columns to datetime format\n",
    "def strip_date(row):\n",
    "    if isinstance(row, str):\n",
    "        row = datetime.strptime(row, '%Y-%m-%d').date()\n",
    "    return row\n",
    "\n",
    "# Apply date conversion to train dataset\n",
    "train['host_since'] = train['host_since'].apply(strip_date)\n",
    "train['first_review'] = train['first_review'].apply(strip_date)\n",
    "train['last_review'] = train['last_review'].apply(strip_date)\n",
    "\n",
    "# Apply date conversion to test dataset\n",
    "test['host_since'] = test['host_since'].apply(strip_date)\n",
    "test['first_review'] = test['first_review'].apply(strip_date)\n",
    "test['last_review'] = test['last_review'].apply(strip_date)\n",
    "\n",
    "# ----- #\n",
    "\n",
    "# Calculate months since various dates for train dataset\n",
    "train['host_since_in_months'] = round(((datetime.now().date() - train['host_since']).dt.days) / 30, 2)\n",
    "train['first_review_in_months'] = round(((datetime.now().date() - train['first_review']).dt.days) / 30, 2)\n",
    "train['last_review_in_months'] = round(((datetime.now().date() - train['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "# Calculate months since various dates for test dataset\n",
    "test['host_since_in_months'] = round(((datetime.now().date() - test['host_since']).dt.days) / 30,  2)\n",
    "test['first_review_in_months'] = round(((datetime.now().date() - test['first_review']).dt.days) / 30, 2)\n",
    "test['last_review_in_months'] = round(((datetime.now().date() - test['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "\n",
    "# Because the review values are extremely collinear, calculate average review scores and fill missing values with 0\n",
    "train['review_scores_avg'] = np.mean(train[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "test['review_scores_avg'] = np.mean(test[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "\n",
    "train['review_scores_avg'].fillna(value=0)\n",
    "test['review_scores_avg'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80f3631b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price outliers: [523, 1626, 1823, 1848, 2067, 2380, 3129, 4865]\n",
      "Min nights outliers: [227, 707, 1748, 2963, 3607]\n",
      "\n",
      "13 observations dropped\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4865       14.0\n",
       "2067       15.0\n",
       "1848       16.0\n",
       "2380       16.0\n",
       "1823     3319.0\n",
       "1626     4500.0\n",
       "523      5000.0\n",
       "3129    99998.0\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Identify outliers in 'price' and 'minimum_nights'\n",
    "\n",
    "# top and bottom 0.04% of price\n",
    "lower_val = np.percentile(train[['price']], 0.07)\n",
    "upper_val = np.percentile(train[['price']], 99.93)\n",
    "outliers_idx_price = list(train[(train['price'] >= upper_val) | (train['price'] <= lower_val)].index)\n",
    "print(\"Price outliers:\", list(outliers_idx_price))\n",
    "\n",
    "\n",
    "# top 0.1% of minimum_nights\n",
    "upper_lim = np.percentile(train[['minimum_nights']], 99.9)\n",
    "outliers_idx_nights = list(train[train['minimum_nights'] >= upper_lim].index)\n",
    "outliers_idx = list(outliers_idx_price) + list(outliers_idx_nights)\n",
    "print(\"Min nights outliers:\", list(train[train['minimum_nights'] >= upper_lim].index))\n",
    "\n",
    "\n",
    "print(f\"\\n{len(train.iloc[outliers_idx, :]['price'])} observations dropped\\n\")\n",
    "train.loc[outliers_idx_price, :]['price'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44a15430",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_cols = ['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights',\n",
    "                   'availability_365', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                   'host_location', 'host_response_time', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'first_review', 'last_review', 'host_since', 'host_neighbourhood', \n",
    "                    'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', ]\n",
    "\n",
    "train_clean = train.drop(outliers_idx).reset_index(drop=True)\n",
    "train_clean.drop(columns=unnecessary_cols, inplace=True)\n",
    "\n",
    "test_clean = test.drop(columns=unnecessary_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75484ac",
   "metadata": {},
   "source": [
    "### Clean/Transform Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e197cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vars(row):\n",
    "    # Check if 'shared' is in 'bathrooms_text' to identify shared bathrooms\n",
    "    if 'shared' in str(row['bathrooms_text']):\n",
    "        row['bathrooms_shared'] = \"t\"\n",
    "        \n",
    "    # Check if 'bathrooms_text' is empty and 'room_type' is 'Shared' to identify shared bathrooms\n",
    "    elif pd.isna(row['bathrooms_text']):\n",
    "        if 'Shared' in row['room_type']:\n",
    "            row['bathrooms_shared'] = \"t\"              \n",
    "        else:\n",
    "            row['bathrooms_shared'] = \"f\"\n",
    "    else: \n",
    "        row['bathrooms_shared'] = \"f\"\n",
    "        \n",
    "    # Convert 'Hotel room' room type to 'Private room'\n",
    "    if row.loc['room_type'] == 'Hotel room':\n",
    "        row['room_type'] = 'Private room'\n",
    "        \n",
    "    return row\n",
    "\n",
    "# Apply the function to clean variables to train and test datasets\n",
    "train_clean = train_clean.apply(clean_vars, axis=1)\n",
    "test_clean = test_clean.apply(clean_vars, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8adb4",
   "metadata": {},
   "source": [
    "##### Clean Neighbourhoods \n",
    "grouping small occurances into the group with the closest mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd006ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group small occurrences into 'Other'\n",
    "neighbourhood_counts = train_clean['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "other_hoods = [i for i in neighbourhood_counts.index if neighbourhood_counts[i] < 100]\n",
    "\n",
    "test_only_hoods = [i for i in test_clean['neighbourhood_cleansed'].unique() \n",
    "                   if i not in neighbourhood_counts \n",
    "                   and i != 'Other']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ac27819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with unique neighbourhoods\n",
    "hood_df = pd.DataFrame(index=train_clean['neighbourhood_cleansed'].unique())\n",
    "\n",
    "# Compute mean and standard deviation for each neighbourhood\n",
    "grouped = train_clean.groupby('neighbourhood_cleansed')['price']\n",
    "all_mean = grouped.mean()\n",
    "all_std = grouped.std()\n",
    "\n",
    "# Add mean and std to DataFrame\n",
    "hood_df['mean_price'] = all_mean\n",
    "hood_df['std_price'] = all_std\n",
    "\n",
    "# Merge with counts\n",
    "hood_df = hood_df.merge(neighbourhood_counts, left_index=True, right_index=True)\n",
    "hood_df.rename(columns={'neighbourhood_cleansed': 'count'}, inplace=True)\n",
    "\n",
    "\n",
    "# Get the 10th percentile of standard deviations\n",
    "std_90 = np.percentile(hood_df.dropna(how='any')['std_price'], 10)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "filtered_df = hood_df[((hood_df['std_price'] < std_90) | (hood_df['count'] > 100)) & (hood_df['count'] > 20)]\n",
    "\n",
    "keep_hoods = filtered_df.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb370a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if neighbourhood has small std or more than 100 but no neighbourhoods with less than 20\n",
    "def clean_hoods(row):\n",
    "    if row.loc['neighbourhood_cleansed'] not in keep_hoods:\n",
    "        row['neighbourhood_grouped'] = 'Other'\n",
    "        \n",
    "    else:    \n",
    "        row['neighbourhood_grouped'] = row.loc['neighbourhood_cleansed']\n",
    "        \n",
    "    return row\n",
    "\n",
    "train_clean = train_clean.apply(clean_hoods, axis=1)\n",
    "test_clean = test_clean.apply(clean_hoods, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c31fc",
   "metadata": {},
   "source": [
    "##### Clean Property Type\n",
    "grouping small occurances into the group with the closest mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e62ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['place', 'room', 'private', 'shared', 'entire', ' in', ' room', ' private', ' shared', ' entire', ' in',]\n",
    "\n",
    "# remove filler and unnecessary words from property\n",
    "def remove_words(text):\n",
    "    text=text.lower()\n",
    "    for word in words_to_remove:\n",
    "        word = word.lower()\n",
    "        text = text.replace(word, '')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "train_clean['property_type'] = train_clean['property_type'].apply(remove_words)\n",
    "test_clean['property_type'] = test_clean['property_type'].apply(remove_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "679ba33f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify value counts and make a list of neighbourhoods with more than 10\n",
    "property_counts = train_clean['property_type'].value_counts()\n",
    "keep = [i for i in property_counts.index if property_counts[i] > 10]\n",
    "\n",
    "def clean_property(row):\n",
    "    if row not in keep or row == \"\":\n",
    "        row = 'Other'\n",
    "      \n",
    "    return row\n",
    "\n",
    "\n",
    "train_clean['property_type_cleansed'] = train_clean['property_type'].apply(clean_property)\n",
    "test_clean['property_type_cleansed'] = test_clean['property_type'].apply(clean_property)\n",
    "\n",
    "train_filter_2 = train_clean.copy()\n",
    "test_filter_2 = test_clean.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb6d2f",
   "metadata": {},
   "source": [
    "### Inspect and impute columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5712a33",
   "metadata": {},
   "source": [
    "#### Model imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb65842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.586516\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       f\n",
       "1       f\n",
       "2       f\n",
       "3       f\n",
       "4       t\n",
       "       ..\n",
       "4982    t\n",
       "4983    t\n",
       "4984    f\n",
       "4985    f\n",
       "4986    f\n",
       "Name: host_is_superhost, Length: 4987, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temporary dataframe to manipulate\n",
    "train_filter_temp = train_filter_2.copy()\n",
    "test_filter_temp = test_filter_2.copy()\n",
    "\n",
    "# Change t/f to numeric 1/0\n",
    "train_filter_temp['host_is_superhost'] = train_filter_2['host_is_superhost'].replace({'f': 0, 't': 1})\n",
    "test_filter_temp['host_is_superhost'] = test_filter_2['host_is_superhost'].replace({'f': 0, 't': 1})\n",
    "\n",
    "# Create model\n",
    "superhost_model = smf.logit(formula=\"host_is_superhost ~ calculated_host_listings_count*number_of_reviews_ltm + response_rate\", data=train_filter_temp).fit()\n",
    "\n",
    "# Predict all values \n",
    "impute_superhost_train = (superhost_model.predict(train_filter_temp) > 0.5).replace({False:'f', True:'t'})\n",
    "impute_superhost_test = (superhost_model.predict(test_filter_temp) > 0.5).replace({False:'f', True:'t'})\n",
    "\n",
    "# fill na's with coordinating value from model imputation\n",
    "train_filter_2['host_is_superhost'].fillna(impute_superhost_train, inplace=True)\n",
    "test_filter_2['host_is_superhost'].fillna(impute_superhost_test, inplace=True)\n",
    "\n",
    "\n",
    "train_filter_2['host_is_superhost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f59ce391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192038\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.067442\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.88\n",
       "1       1.00\n",
       "2       0.99\n",
       "3       1.00\n",
       "4       1.00\n",
       "        ... \n",
       "3333    0.99\n",
       "3334    1.00\n",
       "3335    1.00\n",
       "3336    1.00\n",
       "3337    1.00\n",
       "Name: response_rate, Length: 3338, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model to impute acceptance rate\n",
    "acceptance_model = smf.logit(formula=\"acceptance_rate ~ calculated_host_listings_count + accommodates\", data=train_filter_2).fit()\n",
    "\n",
    "\n",
    "# fill in missing values with the predictions from the model\n",
    "predicted_acceptance = acceptance_model.predict(train_filter_2)\n",
    "train_filter_2['acceptance_rate'].fillna(predicted_acceptance)\n",
    "\n",
    "predicted_acceptance_test = acceptance_model.predict(test_filter_2)\n",
    "test_filter_2['acceptance_rate'].fillna(predicted_acceptance_test)\n",
    "\n",
    "\n",
    "# ----- #\n",
    "\n",
    "\n",
    "# Create model to impute response rate\n",
    "response_model = smf.logit(formula=\"response_rate ~ accommodates\", data=train_filter_2).fit()\n",
    "\n",
    "\n",
    "# fill in missing values with the predictions from the model\n",
    "predicted_response = response_model.predict(train_filter_2)\n",
    "train_filter_2['response_rate'].fillna(predicted_response)\n",
    "\n",
    "predicted_response_test = response_model.predict(test_filter_2)\n",
    "test_filter_2['response_rate'].fillna(predicted_response_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed5943",
   "metadata": {},
   "source": [
    "#### Naive imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d5148e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in remaining missing values with median for numerical columns\n",
    "train_filter_2.fillna(train_filter_2.median(numeric_only=True), inplace=True)\n",
    "test_filter_2.fillna(test_filter_2.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Create final DataFrames\n",
    "train_final = train_filter_2.copy()\n",
    "test_final = test_filter_2.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7b2ba",
   "metadata": {},
   "source": [
    "## 3) Developing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b6517",
   "metadata": {},
   "source": [
    "- Created a dictionary of pairs that are correlated and use VIF to deduce which variables to keep. Some correlated variables were kept to create a more complex model. \n",
    "- For interactions each variable in last_review_in_months, reviews_per_month, number_of_reviews_ltm, review_scores_avg was interacted with each other. \n",
    "- For interactions related to accommodations; bathrooms were transformed by if bathrooms are shared to depend correlation on shared, accommodates interacts with beds as the number of beds and accomodates correlation can indicate if beds are meant to be shared, finally if bathrooms shared with accommodates as higher accommodation is more valuable if bathrooms are not shared.\n",
    "- acceptance_rate, response_rate, and host_is_superhost all interact with each other to transform the acceptance and response rate whether the host is a superhost to quantify how a host being a superhost changes the slope/frequency of the rates\n",
    "- Calculated host listings count interacts with number_of_reviews_ltm, review_scores_avg, and reviews_per_month to capture how the number of listings a host has and the number of reviews are closely connect as well as having more listings at a higher average is an indicator for higher price then few listings or worse reviews. \n",
    "- maximum nights, minimum nights, latitude, and longitude greatly predict price and squared and cubic terms were added to better capture the trend in the connection. calculated host listing count has a large range so the log was taken to standardize further and then higher order terms created. reviews per month and number of reviews ltm both have additional terms according to visualizations and trial and error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002af75",
   "metadata": {},
   "source": [
    "## 4) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d152bf",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64d29d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns used in the model\n",
    "cols_for_sklearn = ['acceptance_rate', 'accommodates', 'availability_30', 'availability_90',\n",
    "                    'bathrooms_num', 'bathrooms_shared', 'beds', \n",
    "                    'calculated_host_listings_count', 'last_review_in_months',\n",
    "                    'host_is_superhost','host_since_in_months', 'latitude', 'longitude', \n",
    "                    'maximum_nights', 'minimum_nights', \n",
    "                    'neighbourhood_grouped', 'number_of_reviews_ltm', \n",
    "                    'price', 'property_type_cleansed', 'response_rate',  \n",
    "                    'review_scores_avg', 'reviews_per_month', 'room_type',\n",
    "                    'maximum_nights_avg_ntm', 'minimum_nights_avg_ntm', 'last_review_in_months']\n",
    "\n",
    "\n",
    "# create list of columns excluding price that can be used for the test data\n",
    "cols_for_sklearn_test = [name for name in cols_for_sklearn if name != 'price']\n",
    "\n",
    "# subset dataset with needed columns\n",
    "subset_train = train_final[cols_for_sklearn].copy()\n",
    "subset_test = test_final[cols_for_sklearn_test].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d08522ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response and predictor objects\n",
    "X_train = subset_train.drop(columns='price')\n",
    "y_train = np.log(subset_train.price)\n",
    "\n",
    "X_test = subset_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df33aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes with dummy variables\n",
    "X_train_preprocessed = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_preprocessed = pd.get_dummies(X_test, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c29fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of the dummy columns created for respective categorical variables\n",
    "room_type_cols = [name for name in X_train_preprocessed.columns if 'room_type' in name]\n",
    "neighbourhood_groups = [name for name in X_train_preprocessed.columns if 'neighbourhood_grouped' in name]\n",
    "property_groups = [name for name in X_train_preprocessed.columns if 'property' in name]\n",
    "\n",
    "\n",
    "# list of pairs of variables to interact\n",
    "interaction_pairs = [('bathrooms_num', 'bathrooms_shared_t'),\n",
    "                     ('beds', 'accommodates'),\n",
    "                     ('accommodates', 'bathrooms_shared_t'),\n",
    "                     \n",
    "                     ('accommodates', 'availability_30'),\n",
    "                     ('accommodates', 'availability_90'),\n",
    "                     ('acceptance_rate', 'host_is_superhost_t'), \n",
    "                     ('response_rate', 'host_is_superhost_t'),\n",
    "                     ('acceptance_rate', 'response_rate'),\n",
    "                     \n",
    "                    ('last_review_in_months', 'reviews_per_month'),\n",
    "                    ('last_review_in_months', 'number_of_reviews_ltm'),\n",
    "                    ('reviews_per_month', 'number_of_reviews_ltm'),\n",
    "                    ('reviews_per_month', 'review_scores_avg'), \n",
    "                    ('number_of_reviews_ltm', 'review_scores_avg'),\n",
    "                     \n",
    "                    ('number_of_reviews_ltm', 'calculated_host_listings_count'),\n",
    "                    ('review_scores_avg', 'calculated_host_listings_count'),\n",
    "                    ('reviews_per_month', 'calculated_host_listings_count')] \n",
    "        \n",
    "        \n",
    "# loop for adding categorical predictors to interaction pairs list\n",
    "for i in room_type_cols:\n",
    "    interaction_pairs.append((i, 'beds'))\n",
    "    interaction_pairs.append((i, 'availability_30'))\n",
    "    \n",
    "    \n",
    "    for j in neighbourhood_groups: \n",
    "        interaction_pairs.append((j, i))\n",
    "    \n",
    "    for k in property_groups:\n",
    "        interaction_pairs.append((i, k))\n",
    "\n",
    "        \n",
    "for i in property_groups:\n",
    "    for j in neighbourhood_groups:\n",
    "        interaction_pairs.append((i, j))\n",
    "\n",
    "        \n",
    "        \n",
    "### ----- ###\n",
    "\n",
    "# list of all the columns used in interactions\n",
    "interaction_cols = []\n",
    "for t in interaction_pairs:\n",
    "    for item in t:\n",
    "        interaction_cols.append(item)\n",
    "        \n",
    "        \n",
    "# initialize dataframe to store interactions\n",
    "interaction_df = pd.DataFrame()\n",
    "interaction_df_test = pd.DataFrame()\n",
    "\n",
    "# for each pair, get the interaction values and add to interaction df\n",
    "for pair in interaction_pairs:\n",
    "    interaction_columns = pair\n",
    "    \n",
    "    X_train_interaction_subset = X_train_preprocessed[list(interaction_columns)]\n",
    "    X_test_interaction_subset = X_test_preprocessed[list(interaction_columns)]\n",
    "    \n",
    "    poly_features = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    interaction_terms_train = poly_features.fit_transform(X_train_interaction_subset)\n",
    "    interaction_terms_test = poly_features.transform(X_test_interaction_subset)\n",
    "    \n",
    "    interaction_column_names = poly_features.get_feature_names_out()\n",
    "    \n",
    "    interaction_df_pair = pd.DataFrame(interaction_terms_train, columns=interaction_column_names)\n",
    "    interaction_df_pair_test = pd.DataFrame(interaction_terms_test, columns=interaction_column_names)\n",
    "    \n",
    "\n",
    "    interaction_df = pd.concat([interaction_df, interaction_df_pair], axis=1)\n",
    "    interaction_df_test = pd.concat([interaction_df_test, interaction_df_pair_test], axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e3286f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add interaction data frame to main dataset and drop any duplicated columns\n",
    "X_train_processed = pd.concat([X_train_preprocessed.drop(columns=interaction_cols), interaction_df], axis=1)\n",
    "X_test_processed = pd.concat([X_test_preprocessed.drop(columns=interaction_cols), interaction_df_test], axis=1)        \n",
    "\n",
    "X_train_processed = X_train_processed.loc[:,~X_train_processed.columns.duplicated()].copy()\n",
    "X_train_processed = X_train_processed.reindex(sorted(X_train_processed.columns), axis=1)\n",
    "X_test_processed = X_test_processed.loc[:,~X_test_processed.columns.duplicated()].copy()\n",
    "X_test_processed = X_test_processed.reindex(sorted(X_test_processed.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e627456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add higher order terms for select predictors in both the test and training data\n",
    "X_train_processed['maximum_nights^2'] = X_train_processed['maximum_nights'] ** 2\n",
    "X_train_processed['maximum_nights^3'] = X_train_processed['maximum_nights'] ** 3\n",
    "X_train_processed['minimum_nights^2'] = X_train_processed['minimum_nights'] ** 2\n",
    "X_train_processed['minimum_nights^3'] = X_train_processed['minimum_nights'] ** 3\n",
    "X_train_processed['longitude^2'] = X_train_processed['longitude'] ** 2\n",
    "X_train_processed['latitude^2'] = X_train_processed['latitude'] ** 2\n",
    "X_train_processed['longitude^3'] = X_train_processed['longitude'] ** 3\n",
    "X_train_processed['latitude^3'] = X_train_processed['latitude'] ** 3\n",
    "X_train_processed['calculated_host_listings_count_log'] = np.log(X_train_processed['calculated_host_listings_count'])\n",
    "X_train_processed['calculated_host_listings_count_log^2'] = np.log(X_train_processed['calculated_host_listings_count'])**2\n",
    "X_train_processed['calculated_host_listings_count_log^3'] = np.log(X_train_processed['calculated_host_listings_count'])**3\n",
    "X_train_processed['reviews_per_month^2'] = X_train_processed['reviews_per_month'] ** 2\n",
    "X_train_processed['number_of_reviews_ltm_root'] = np.sqrt(X_train_processed['number_of_reviews_ltm'])\n",
    "\n",
    "X_test_processed['maximum_nights^2'] = X_test_processed['maximum_nights'] ** 2\n",
    "X_test_processed['maximum_nights^3'] = X_test_processed['maximum_nights'] ** 3\n",
    "X_test_processed['minimum_nights^2'] = X_test_processed['minimum_nights'] ** 2\n",
    "X_test_processed['minimum_nights^3'] = X_test_processed['minimum_nights'] ** 3\n",
    "X_test_processed['longitude^2'] = X_test_processed['longitude'] ** 2\n",
    "X_test_processed['latitude^2'] = X_test_processed['latitude'] ** 2\n",
    "X_test_processed['longitude^3'] = X_test_processed['longitude'] ** 3\n",
    "X_test_processed['latitude^3'] = X_test_processed['latitude'] ** 3\n",
    "X_test_processed['calculated_host_listings_count_log'] = np.log(X_test_processed['calculated_host_listings_count'])\n",
    "X_test_processed['calculated_host_listings_count_log^2'] = np.log(X_test_processed['calculated_host_listings_count'])**2\n",
    "X_test_processed['calculated_host_listings_count_log^3'] = np.log(X_test_processed['calculated_host_listings_count'])**3\n",
    "X_test_processed['reviews_per_month^2'] = X_test_processed['reviews_per_month'] ** 2\n",
    "X_test_processed['number_of_reviews_ltm_root'] = np.sqrt(X_test_processed['number_of_reviews_ltm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65a4284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series of loops to create new categorical column and numerical column lists to account for the new dummy and interaction columns\n",
    "cat_cols = X_train.select_dtypes(exclude='number').columns\n",
    "cat_dummy_dict = {}\n",
    "\n",
    "# Creating dummy variables for each categorical feature\n",
    "for cat in cat_cols:\n",
    "    dummy_df = pd.get_dummies(X_train[cat], prefix=cat, drop_first=True)\n",
    "    cat_dummy_dict[cat] = list(dummy_df.columns)\n",
    "\n",
    "# Generating combinations of categorical dummy variables\n",
    "possible_cat_cat = []\n",
    "for k, v in cat_dummy_dict.items():\n",
    "    for j, w in cat_dummy_dict.items():\n",
    "        if k != j:\n",
    "            possible_cat_cat.extend([f\"{x} {y}\" for x in v for y in w])\n",
    "\n",
    "# Creating a list of all dummy variables\n",
    "plain_dummies = list(pd.get_dummies(X_train[cat_cols], drop_first=True).columns)\n",
    "\n",
    "# Combining all dummy variables\n",
    "binary_cols = plain_dummies + possible_cat_cat\n",
    "\n",
    "# Filtering categorical and numerical columns\n",
    "new_cat_cols = [col for col in X_train_processed.columns if col in binary_cols]\n",
    "new_num_cols = [col for col in X_train_processed.columns if col not in new_cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fd0b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit scaler object\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train_processed[new_num_cols])\n",
    "\n",
    "# reassign the numerical columns to their scaled value\n",
    "X_train_processed[new_num_cols] = sc.transform(X_train_processed[new_num_cols])\n",
    "X_test_processed[new_num_cols] = sc.transform(X_test_processed[new_num_cols]) \n",
    "\n",
    "X_train_final = X_train_processed.copy()\n",
    "X_test_final = X_test_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c951889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit model using transformed and scaled data\n",
    "lrm = LinearRegression()\n",
    "lrm.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54aa9f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.7342\n",
      "51.5415\n",
      "Diff rmse-mae: 61.1927\n"
     ]
    }
   ],
   "source": [
    "# Predict the train data values with the model and calculate training rmse and mae \n",
    "y_pred_train = np.exp(lrm.predict(X_train_final))*1.05\n",
    "rmse = mean_squared_error(train_final.price, y_pred_train, squared = False) \n",
    "mae = mean_absolute_error(train_final.price, y_pred_train) \n",
    "\n",
    "print(round(rmse, 4))\n",
    "print(round(mae, 4))\n",
    "print(\"Diff rmse-mae:\", round(rmse-mae, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7022e516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict price in the test data\n",
    "## Convert to true value with np.exp() and adjust with 1.05 coefficient\n",
    "predicted_values = pd.DataFrame(np.exp(lrm.predict(X_test_final))*1.05, columns=['predicted'])\n",
    "\n",
    "# add listing id to the predicted values dataframe and set the index to the id value\n",
    "predicted_values = predicted_values.merge(test_final['id'], left_index=True, right_index=True).set_index('id').rename(columns={0:'predicted'})\n",
    "predicted_values\n",
    "\n",
    "predicted_values.to_csv('liner_model_predicted_values.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
