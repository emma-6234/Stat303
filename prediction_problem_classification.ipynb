{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f0a480d3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6a1a7",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824f269",
   "metadata": {},
   "source": [
    "- First, there were columns that had characters in them (%, \\$) so I filtered cleaned the columns to be numeric. \n",
    "- There were different variations to quantify the number of listings a host has, so using correlations and some trial and error only one of the host listing count predictors was selected. \n",
    "- Host response time was categorical so I converted the categorical values to approx. numeric values\n",
    "- Review scores are averaged because they are very highly correlated.\n",
    "- There were some columns that were dates so they were converted to datetime objects and also a column was created for 'months since' that date to make the date columns easier to utilize\n",
    "- Property types and Neighbourhoods had some observations with low occurances so those were grouped into 'Other'\n",
    "- missing values were imputed naively with the columns median value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0607077",
   "metadata": {},
   "source": [
    "## 2) Data Cleaning/Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd80240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac82fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('datasets/train_classification.csv')\n",
    "raw_test = pd.read_csv('datasets/test_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0493a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the raw datasets\n",
    "train = raw_train.copy()\n",
    "test = raw_test.copy()\n",
    "\n",
    "# Clean 'price' column: remove '$' and ',' characters, and convert to float\n",
    "train['acceptance_rate'] = train['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "train['response_rate'] = train['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "test['acceptance_rate'] = test['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "test['response_rate'] = test['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "test.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "\n",
    "\n",
    "# Extract numeric values from 'bathrooms_text' column and convert to float\n",
    "train['bathrooms_num'] = train['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "test['bathrooms_num'] = test['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Fill missing values in 'bathrooms_num' where 'Half-bath' is mentioned in 'bathrooms_text' with 0.5\n",
    "train.loc[train['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & train['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n",
    "test.loc[test['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & test['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc94a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "def strip_date(row):\n",
    "    if isinstance(row, str):\n",
    "        row = datetime.strptime(row, '%Y-%m-%d').date()\n",
    "    return row\n",
    "\n",
    "# Apply date conversion to train dataset\n",
    "train['host_since'] = train['host_since'].apply(strip_date)\n",
    "train['first_review'] = train['first_review'].apply(strip_date)\n",
    "train['last_review'] = train['last_review'].apply(strip_date)\n",
    "\n",
    "# Apply date conversion to test dataset\n",
    "test['host_since'] = test['host_since'].apply(strip_date)\n",
    "test['first_review'] = test['first_review'].apply(strip_date)\n",
    "test['last_review'] = test['last_review'].apply(strip_date)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate months since various dates for train dataset\n",
    "train['host_since_in_months'] = round(((datetime.now().date() - train['host_since']).dt.days) / 30, 2)\n",
    "train['first_review_in_months'] = round(((datetime.now().date() - train['first_review']).dt.days) / 30, 2)\n",
    "train['last_review_in_months'] = round(((datetime.now().date() - train['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "# Calculate months since various dates for test dataset\n",
    "test['host_since_in_months'] = round(((datetime.now().date() - test['host_since']).dt.days) / 30,  2)\n",
    "test['first_review_in_months'] = round(((datetime.now().date() - test['first_review']).dt.days) / 30, 2)\n",
    "test['last_review_in_months'] = round(((datetime.now().date() - test['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "\n",
    "train_clean = train.copy() \n",
    "test_clean = test.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba25018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for response time category conversions\n",
    "response_time_dict = {'within an hour': 1, 'within a few hours': 12, 'within a day': 24, 'a few days or more': 72}\n",
    "\n",
    "def replace_response_time(row):\n",
    "    if pd.notna(row):\n",
    "        return response_time_dict.get(row)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train['response_time'] = train['host_response_time'].apply(replace_response_time)\n",
    "test['response_time'] = test['host_response_time'].apply(replace_response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de0269",
   "metadata": {},
   "source": [
    "### Clean/Transform Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629108ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group hotel room into private room because there is a low number of occurances for hotel\n",
    "def clean_rooms(row):    \n",
    "    if row.loc['room_type'] == 'Hotel room':\n",
    "        row['room_type'] = 'Private room'\n",
    "        \n",
    "    return row\n",
    "\n",
    "train_clean = train_clean.apply(clean_rooms, axis=1)\n",
    "test_clean = test_clean.apply(clean_rooms, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b311ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for rate of reviews for listing count and for host_since_months\n",
    "train_clean['reviews_per_listing'] = train_clean['number_of_reviews']/train_clean['calculated_host_listings_count']\n",
    "train_clean['reviews_per_month'] = train_clean['number_of_reviews']/train_clean['host_since_in_months']\n",
    "train_clean['reviews_per_listing_per_month'] = train_clean['reviews_per_listing']/train_clean['host_since_in_months']\n",
    "\n",
    "test_clean['reviews_per_listing'] = test_clean['number_of_reviews']/test_clean['calculated_host_listings_count']\n",
    "test_clean['reviews_per_month'] = test_clean['number_of_reviews']/test_clean['host_since_in_months']\n",
    "test_clean['reviews_per_listing_per_month'] = test_clean['reviews_per_listing']/test_clean['host_since_in_months']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17ae87",
   "metadata": {},
   "source": [
    "#### Sophisticated Cleaning of Neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37085991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if neighbourhood has less than 150 occurances group them into 'Other'\n",
    "neighbourhood_counts = train_clean['neighbourhood_cleansed'].value_counts()  \n",
    "test_only_hoods = [i for i in test_clean['neighbourhood_cleansed'].unique() \n",
    "                   if i not in neighbourhood_counts \n",
    "                   and i != 'Other']\n",
    "\n",
    "\n",
    "other_hoods = []\n",
    "for i in neighbourhood_counts.index:\n",
    "    if neighbourhood_counts[i] < 150:\n",
    "        other_hoods.append(i)         \n",
    "    \n",
    "def clean_hoods(row):\n",
    "    if row.loc['neighbourhood_cleansed'] in other_hoods or row.loc['neighbourhood_cleansed'] in test_only_hoods:\n",
    "        row['neighbourhood_grouped'] = 'Other'\n",
    "        \n",
    "    else:    \n",
    "        row['neighbourhood_grouped'] = row.loc['neighbourhood_cleansed']\n",
    "        \n",
    "    return row\n",
    "   \n",
    "    \n",
    "train_clean = train_clean.apply(clean_hoods, axis=1)  \n",
    "test_clean = test_clean.apply(clean_hoods, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c9ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean filler words out of property types\n",
    "words_to_remove = ['place', 'room', 'private', 'shared', 'entire', ' in', ' room', ' private', ' shared', ' entire', ' in',]\n",
    "\n",
    "def remove_words(text):\n",
    "    text=text.lower()\n",
    "    for word in words_to_remove:\n",
    "        word = word.lower()\n",
    "        text = text.replace(word, '')\n",
    "    return text.strip()\n",
    "\n",
    "train_clean['property_type'] = train_clean['property_type'].apply(remove_words)\n",
    "test_clean['property_type'] = test_clean['property_type'].apply(remove_words)\n",
    "\n",
    "\n",
    "# group properties with less than 10 occurances into 'Other'\n",
    "property_counts = train_clean['property_type'].value_counts()\n",
    "keep = [i for i in property_counts.index if property_counts[i] > 10]\n",
    "\n",
    "def clean_property(row):\n",
    "    if row not in keep or row == \"\":\n",
    "        row = 'Other'\n",
    "      \n",
    "    return row\n",
    "\n",
    "train_clean['property_type_cleansed'] = train_clean['property_type'].apply(clean_property)\n",
    "test_clean['property_type_cleansed'] = test_clean['property_type'].apply(clean_property)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b1a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in remaining missing values with median for numerical columns\n",
    "train_clean.fillna(train_clean.median(numeric_only=True), inplace=True)\n",
    "train_clean.fillna(train_clean.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Create final DataFrames\n",
    "train_final = train_clean.copy()\n",
    "test_final = test_clean.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972c92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review scores are very correlated, average review scores to handle this\n",
    "train_final['review_scores_avg'] = np.mean(train_final[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "test_final['review_scores_avg'] = np.mean(test_final[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "\n",
    "train_final['review_scores_avg'] = train_final['review_scores_avg'].fillna(value=train_final['review_scores_avg'].median())\n",
    "test_final['review_scores_avg'] = test_final['review_scores_avg'].fillna(value=train_final['review_scores_avg'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80160566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace f/t values in train final with numeric 0/1\n",
    "train_final['host_is_superhost'] = train_final['host_is_superhost'].replace({\"f\":0, \"t\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae544b",
   "metadata": {},
   "source": [
    "## 3) Developing the Model\n",
    "\n",
    "Host experience, number of listings, and number of reviews are all influence and have some quadratic relation to host_is_superhost so those predictors are squared. Number of reviews depends on the host's number of listings so these variables interact, similiarly host listing count is connected to reviews per month. How long someone has been a host is connected to their number of listings so those variables interact. \n",
    "Review scores average is connected to number of reviews ltm because as there are more reviews, the average is more reliable as well as more reviews that are higher may indicate a host is more likely to be a superhost. Certain neighbourhoods may require more experienced/high-level host's so neighbourhood is added. Lastly reviews scores average has higher order terms because reviews are direclty and strongly correlated to superhosts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e60c7",
   "metadata": {},
   "source": [
    "## 4) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9280469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.470835\n",
      "         Iterations 18\n"
     ]
    }
   ],
   "source": [
    "formula= '''host_is_superhost ~\n",
    "C(response_time) + \n",
    "\n",
    "I(host_since_in_months**2) + I(host_total_listings_count**2) +\n",
    "I(number_of_reviews_ltm**2) +\n",
    "\n",
    "host_total_listings_count*number_of_reviews_ltm + \n",
    "host_since_in_months*host_total_listings_count + \n",
    "host_since_in_months*reviews_per_listing + \n",
    "host_total_listings_count*reviews_per_month +\n",
    "\n",
    "review_scores_avg*number_of_reviews_ltm + \n",
    "review_scores_avg + I(review_scores_avg**2) + I(review_scores_avg**3) + \n",
    "\n",
    "C(neighbourhood_grouped)'''\n",
    "\n",
    "model = smf.logit(formula=formula, data=train_final).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5c30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 0.488 77.6773\n"
     ]
    }
   ],
   "source": [
    "# find the optimal threshold for the logistic model\n",
    "thresholds = np.linspace(0.4, 0.6, num=201)\n",
    "\n",
    "accuracies = []\n",
    "for a in thresholds:\n",
    "    y_pred = model.predict(train_final) > a\n",
    "    accuracies.append(accuracy_score(train_final.host_is_superhost, y_pred))\n",
    "\n",
    "max_idx = np.argmax(accuracies)    \n",
    "    \n",
    "print(max_idx, round(thresholds[max_idx], 6), round(100*np.max(accuracies), 4))\n",
    "\n",
    "optimal_threshold = thresholds[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd309ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    3046\n",
      "True     1931\n",
      "dtype: int64\n",
      "\n",
      "Accuracy: 77.6773\n",
      "Precision: 77.7835\tRecall: 68.7729\n",
      "FPR: 15.3598\t\tFNR: 31.2271\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2364  429]\n",
      " [ 682 1502]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict values and compute metrics for the training data\n",
    "y_pred = model.predict(train_final) > optimal_threshold\n",
    "\n",
    "print(y_pred.value_counts())\n",
    "\n",
    "acc_score = accuracy_score(train_final.host_is_superhost, y_pred)*100\n",
    "precision = precision_score(train_final.host_is_superhost, y_pred)*100\n",
    "recall = recall_score(train_final.host_is_superhost, y_pred)*100\n",
    "conf_matrix = confusion_matrix(train_final.host_is_superhost, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "FPR = fp/(fp + tn)\n",
    "FNR = fn/(fn + tp)\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy: {round(acc_score, 4)}\")\n",
    "print(f\"Precision: {round(precision, 4)}\\tRecall: {round(recall, 4)}\")\n",
    "print(f\"FPR: {round(FPR*100, 4)}\\t\\tFNR: {round(FNR*100, 4)}\\n\")\n",
    "    \n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "\n",
    "    \n",
    "last_acc_score = acc_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e98c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prediction\n",
    "test_pred = (model.predict(test_final) > optimal_threshold).replace({False:0, True:1})\n",
    "predicted_values = pd.concat([test_final[['id', 'host_id']], test_pred.rename('predicted')], axis=1)\n",
    "\n",
    "# hosts that are in both dataframes\n",
    "overlapping_hosts = train_final[train_final['host_id'].isin(test_final['host_id'])].drop_duplicates('host_id')[['host_id', 'host_is_superhost']]\n",
    "\n",
    "# if a host is in both dataframes, overwrite the predicted value with the value from the training data\n",
    "def overwrite(row):\n",
    "    if row['host_id'] in overlapping_hosts['host_id'].values:\n",
    "        row['predicted'] = overlapping_hosts[overlapping_hosts['host_id'] == row['host_id']]['host_is_superhost'].values[0]\n",
    "    return row\n",
    "        \n",
    "    \n",
    "predicted_values = predicted_values.apply(overwrite, axis=1)\n",
    "predicted_values = predicted_values[['id', 'predicted']].set_index('id')\n",
    "\n",
    "predicted_values.to_csv('classification_model_predicted_values.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
