{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (KNN; Classification)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b9670",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "- This is the template for the code and report on the Prediction Problem assignments.\n",
    "\n",
    "- Your code in steps 1, 3, 4, and 5 will be executed sequentially, and must produce the RMSE / accuracy claimed on Kaggle.\n",
    "\n",
    "- Your code in step 2 will also be executed, and must produce the optimal hyperparameter values used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1dc0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, mean_squared_error\n",
    "from scipy.stats import uniform\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('../Datasets/train_classification.csv') # use reg data for class?\n",
    "raw_test = pd.read_csv('../Datasets/test_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc06dc",
   "metadata": {},
   "source": [
    "Put the data pre-processing code. You don't need to explain it. You may use the same code from last quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2dc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the raw datasets\n",
    "train = raw_train.copy()\n",
    "test = raw_test.copy()\n",
    "\n",
    "# Convert 'host_acceptance_rate' and 'host_response_rate' columns to float and scale by dividing by 100\n",
    "train['acceptance_rate'] = train['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "train['response_rate'] = train['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "test['acceptance_rate'] = test['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "test['response_rate'] = test['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "test.drop(columns=['host_acceptance_rate', 'host_response_rate'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Extract numeric values from 'bathrooms_text' column and convert to float\n",
    "train['bathrooms_num'] = train['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "test['bathrooms_num'] = test['bathrooms_text'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Fill missing values in 'bathrooms_num' where 'Half-bath' is mentioned in 'bathrooms_text' with 0.5\n",
    "train.loc[train['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & train['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n",
    "test.loc[test['bathrooms_text'].str.contains('Half-bath', case=False, na=False) & test['bathrooms_num'].isna(), 'bathrooms_num'] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aabab82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "def strip_date(row):\n",
    "    if isinstance(row, str):\n",
    "        row = datetime.strptime(row, '%Y-%m-%d').date()\n",
    "    return row\n",
    "\n",
    "# Apply date conversion to train dataset\n",
    "train['host_since'] = train['host_since'].apply(strip_date)\n",
    "train['first_review'] = train['first_review'].apply(strip_date)\n",
    "train['last_review'] = train['last_review'].apply(strip_date)\n",
    "\n",
    "# Apply date conversion to test dataset\n",
    "test['host_since'] = test['host_since'].apply(strip_date)\n",
    "test['first_review'] = test['first_review'].apply(strip_date)\n",
    "test['last_review'] = test['last_review'].apply(strip_date)\n",
    "\n",
    "# ----- #\n",
    "\n",
    "# Calculate months since various dates for train dataset\n",
    "train['host_since_in_months'] = round(((datetime.now().date() - train['host_since']).dt.days) / 30, 2)\n",
    "train['first_review_in_months'] = round(((datetime.now().date() - train['first_review']).dt.days) / 30, 2)\n",
    "train['last_review_in_months'] = round(((datetime.now().date() - train['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "# Calculate months since various dates for test dataset\n",
    "test['host_since_in_months'] = round(((datetime.now().date() - test['host_since']).dt.days) / 30,  2)\n",
    "test['first_review_in_months'] = round(((datetime.now().date() - test['first_review']).dt.days) / 30, 2)\n",
    "test['last_review_in_months'] = round(((datetime.now().date() - test['last_review']).dt.days) / 30, 2)\n",
    "\n",
    "\n",
    "train_clean = train.copy()\n",
    "test_clean = test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9712b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for response time category conversions\n",
    "response_time_dict = {'within an hour': 1, 'within a few hours': 12, 'within a day': 24, 'a few days or more': 72}\n",
    "\n",
    "def replace_response_time(row):\n",
    "    if pd.notna(row):\n",
    "        return response_time_dict.get(row)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train['response_time'] = train['host_response_time'].apply(replace_response_time)\n",
    "test['response_time'] = test['host_response_time'].apply(replace_response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb25d2",
   "metadata": {},
   "source": [
    "Clean Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3583ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vars(row):\n",
    "    # Check if 'shared' is in 'bathrooms_text' to identify shared bathrooms\n",
    "    if 'shared' in str(row['bathrooms_text']):\n",
    "        row['bathrooms_shared'] = \"t\"\n",
    "        \n",
    "    # Check if 'bathrooms_text' is empty and 'room_type' is 'Shared' to identify shared bathrooms\n",
    "    elif pd.isna(row['bathrooms_text']):\n",
    "        if 'Shared' in row['room_type']:\n",
    "            row['bathrooms_shared'] = \"t\"              \n",
    "        else:\n",
    "            row['bathrooms_shared'] = \"f\"\n",
    "    else: \n",
    "        row['bathrooms_shared'] = \"f\"\n",
    "        \n",
    "    # Convert 'Hotel room' room type to 'Private room'\n",
    "    if row.loc['room_type'] == 'Hotel room':\n",
    "        row['room_type'] = 'Private room'\n",
    "        \n",
    "    return row\n",
    "\n",
    "# Apply the function to clean variables to train and test datasets\n",
    "train_clean = train_clean.apply(clean_vars, axis=1)\n",
    "test_clean = test_clean.apply(clean_vars, axis=1)\n",
    "\n",
    "\n",
    "# create variables for rate of reviews for listing count and for host_since_months\n",
    "train_clean['reviews_per_listing'] = train_clean['number_of_reviews']/train_clean['calculated_host_listings_count']\n",
    "train_clean['reviews_per_month'] = train_clean['number_of_reviews']/train_clean['host_since_in_months']\n",
    "train_clean['reviews_per_listing_per_month'] = train_clean['reviews_per_listing']/train_clean['host_since_in_months']\n",
    "\n",
    "test_clean['reviews_per_listing'] = test_clean['number_of_reviews']/test_clean['calculated_host_listings_count']\n",
    "test_clean['reviews_per_month'] = test_clean['number_of_reviews']/test_clean['host_since_in_months']\n",
    "test_clean['reviews_per_listing_per_month'] = test_clean['reviews_per_listing']/test_clean['host_since_in_months']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df51a6",
   "metadata": {},
   "source": [
    "clean neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75065138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if neighbourhood has less than 150 occurances group them into 'Other'\n",
    "neighbourhood_counts = train_clean['neighbourhood_cleansed'].value_counts()  \n",
    "test_only_hoods = [i for i in test_clean['neighbourhood_cleansed'].unique() \n",
    "                   if i not in neighbourhood_counts \n",
    "                   and i != 'Other']\n",
    "\n",
    "\n",
    "other_hoods = []\n",
    "for i in neighbourhood_counts.index:\n",
    "    if neighbourhood_counts[i] < 150:\n",
    "        other_hoods.append(i)         \n",
    "    \n",
    "def clean_hoods(row):\n",
    "    if row.loc['neighbourhood_cleansed'] in other_hoods or row.loc['neighbourhood_cleansed'] in test_only_hoods:\n",
    "        row['neighbourhood_grouped'] = 'Other'\n",
    "        \n",
    "    else:    \n",
    "        row['neighbourhood_grouped'] = row.loc['neighbourhood_cleansed']\n",
    "        \n",
    "    return row\n",
    "   \n",
    "    \n",
    "train_clean = train_clean.apply(clean_hoods, axis=1)  \n",
    "test_clean = test_clean.apply(clean_hoods, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43849aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean filler words out of property types\n",
    "words_to_remove = ['place', 'room', 'private', 'shared', 'entire', ' in', ' room', ' private', ' shared', ' entire', ' in',]\n",
    "\n",
    "def remove_words(text):\n",
    "    text=text.lower()\n",
    "    for word in words_to_remove:\n",
    "        word = word.lower()\n",
    "        text = text.replace(word, '')\n",
    "    return text.strip()\n",
    "\n",
    "train_clean['property_type'] = train_clean['property_type'].apply(remove_words)\n",
    "test_clean['property_type'] = test_clean['property_type'].apply(remove_words)\n",
    "\n",
    "\n",
    "# group properties with less than 10 occurances into 'Other'\n",
    "property_counts = train_clean['property_type'].value_counts()\n",
    "keep = [i for i in property_counts.index if property_counts[i] > 10]\n",
    "\n",
    "def clean_property(row):\n",
    "    if row not in keep or row == \"\":\n",
    "        row = 'Other'\n",
    "      \n",
    "    return row\n",
    "\n",
    "train_clean['property_type_cleansed'] = train_clean['property_type'].apply(clean_property)\n",
    "test_clean['property_type_cleansed'] = test_clean['property_type'].apply(clean_property)\n",
    "\n",
    "train_filter_2 = train_clean.copy()\n",
    "test_filter_2 = test_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8cea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_filter_2['host_verifications'] = train_filter_2['host_verifications'].apply(ast.literal_eval)\n",
    "except: pass\n",
    "try:\n",
    "    test_filter_2['host_verifications'] = test_filter_2['host_verifications'].apply(ast.literal_eval)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f9d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter_2['num_verifications'] = train_filter_2['host_verifications'].apply(len)\n",
    "test_filter_2['num_verifications'] = test_filter_2['host_verifications'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vers(df):\n",
    "    def update_verification(row):\n",
    "        ver_phone = 't' if 'phone' in row['host_verifications'] else 'f'\n",
    "        ver_email = 't' if 'email' in row['host_verifications'] else 'f'\n",
    "        ver_work_email = 't' if 'work_email' in row['host_verifications'] else 'f'\n",
    "        return pd.Series({'ver_phone': ver_phone, 'ver_email': ver_email, 'ver_work_email': ver_work_email})\n",
    "\n",
    "    df[['ver_phone', 'ver_email', 'ver_work_email']] = df.apply(update_verification, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_filter_2 = split_vers(train_filter_2).drop('host_verifications', axis=1)\n",
    "test_filter_2 = split_vers(test_filter_2).drop('host_verifications', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20800114",
   "metadata": {},
   "source": [
    "Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in remaining missing values with median for numerical columns\n",
    "train_clean.fillna(train_clean.median(numeric_only=True), inplace=True)\n",
    "train_clean.fillna(train_clean.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Create final DataFrames\n",
    "train_final = train_clean.copy()\n",
    "test_final = test_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87270ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review scores are very correlated, average review scores to handle this\n",
    "train_final['review_scores_avg'] = np.mean(train_final[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "test_final['review_scores_avg'] = np.mean(test_final[['review_scores_rating', 'review_scores_value', 'review_scores_location', 'review_scores_cleanliness']], axis=1)\n",
    "\n",
    "train_final['review_scores_avg'] = train_final['review_scores_avg'].fillna(value=train_final['review_scores_avg'].median())\n",
    "test_final['review_scores_avg'] = test_final['review_scores_avg'].fillna(value=train_final['review_scores_avg'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7fc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final['host_is_superhost'] = train_final['host_is_superhost'].replace({\"f\":0, \"t\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "**Paste the hyperparameter tuning code below. You must show at least one hyperparameter tuning procedure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efccf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "**Paste the optimal hyperparameter values below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33497b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a37864",
   "metadata": {},
   "source": [
    "Using the optimal model hyperparameters, train the model, and paste the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, GridSearchCV, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "077d1735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmal\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:227: UserWarning: Found unknown categories in columns [0, 1, 3, 7, 8, 10, 12, 13] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = train_final.select_dtypes(include=['number']).drop(columns=['host_is_superhost', 'id']).columns\n",
    "\n",
    "X_train = train_final.drop(columns=['host_is_superhost', 'id'])\n",
    "X_test = test_final.drop(columns=['id'])\n",
    "train_final\n",
    "X_train_num = X_train[numeric_columns]\n",
    "y_train = train_final.host_is_superhost\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train_num)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train[numeric_columns])\n",
    "X_test_scaled = sc.transform(X_test[numeric_columns])\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(X_train_scaled, columns=numeric_columns)\n",
    "X_test_num_scaled = pd.DataFrame(X_test_scaled, columns=numeric_columns)\n",
    "\n",
    "\n",
    "\n",
    "train_testing = train_final.drop(columns=['host_is_superhost']) # 'host_location', 'host_neighbourhood', \n",
    "test_testing = test_final  #.drop(columns=['host_location', 'host_neighbourhood'])\n",
    "\n",
    "train_testing_cat = train_testing.select_dtypes(exclude=['number'])\n",
    "test_testing_cat = test_testing.select_dtypes(exclude=['number'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(drop='if_binary', handle_unknown='ignore')\n",
    "enc.fit(train_testing_cat)\n",
    "\n",
    "drop_enc = enc.transform(train_testing_cat)\n",
    "drop_enc_test = enc.transform(test_testing_cat)\n",
    "\n",
    "train_encoded_df = pd.DataFrame(drop_enc.toarray(), columns=enc.get_feature_names_out(train_testing_cat.columns))\n",
    "test_encoded_df = pd.DataFrame(drop_enc_test.toarray(), columns=enc.get_feature_names_out(test_testing_cat.columns))\n",
    "\n",
    "X_train_final = pd.concat([X_train_num_scaled, train_encoded_df], axis=1)\n",
    "X_test_final = pd.concat([X_test_num_scaled, test_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7084c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_power_2(distance):\n",
    "    return 1/(1e-10+distance**2)\n",
    "def dist_power_3(distance):\n",
    "    return 1/(1e-10+distance**3)\n",
    "def dist_power_4(distance):\n",
    "    return 1/(1e-10+distance**4)\n",
    "def dist_power_5(distance):\n",
    "    return 1/(1e-10+distance**5)\n",
    "\n",
    "def dist_power_6(distance):\n",
    "    return 1/(1e-10+distance**6)\n",
    "def dist_power_7(distance):\n",
    "    return 1/(1e-10+distance**7)\n",
    "def dist_power_8(distance):\n",
    "    return 1/(1e-10+distance**8)\n",
    "def dist_power_9(distance):\n",
    "    return 1/(1e-10+distance**9)\n",
    "def dist_power_10(distance):\n",
    "    return 1/(1e-10+distance**10)\n",
    "\n",
    "def dist_power_11(distance):\n",
    "    return 1/(1e-10+distance**11)\n",
    "def dist_power_12(distance):\n",
    "    return 1/(1e-10+distance**12)\n",
    "def dist_power_13(distance):\n",
    "    return 1/(1e-10+distance**13)\n",
    "def dist_power_14(distance):\n",
    "    return 1/(1e-10+distance**14)\n",
    "def dist_power_15(distance):\n",
    "    return 1/(1e-10+distance**15)\n",
    "\n",
    "def dist_power_16(distance):\n",
    "    return 1/(1e-10+distance**16)\n",
    "def dist_power_17(distance):\n",
    "    return 1/(1e-10+distance**17)\n",
    "def dist_power_18(distance):\n",
    "    return 1/(1e-10+distance**18)\n",
    "def dist_power_19(distance):\n",
    "    return 1/(1e-10+distance**19)\n",
    "def dist_power_20(distance):\n",
    "    return 1/(1e-10+distance**20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115bb36",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_settings_naive = StratifiedKFold(n_splits=3, shuffle=True, random_state=12)\n",
    "\n",
    "step_1 = 10\n",
    "range_1 = 100\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "grid = {'n_neighbors':np.arange(step_1, step_1+range_1, step_1), 'weights':[dist_power_2, dist_power_3, dist_power_4, dist_power_5, dist_power_6, dist_power_7, dist_power_8, dist_power_9, dist_power_10, dist_power_11, dist_power_12, dist_power_13, dist_power_14, dist_power_15, dist_power_16]}\n",
    "\n",
    "rscv = RandomizedSearchCV(model, grid, n_iter=50, cv=cv_settings_naive, verbose=2)\n",
    "rscv.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc31355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(rscv\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(rscv\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(rscv.best_params_)\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099a000",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bae4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_k = rscv.best_params_['n_neighbors']\n",
    "gscv_step = 5\n",
    "search_radii = int(step_1 + np.ceil(step_1/4))\n",
    "\n",
    "cv_settings = StratifiedKFold(n_splits=5, shuffle=True, random_state=12) \n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "grid = {'n_neighbors':np.arange(prev_k-search_radii, prev_k+search_radii+gscv_step, gscv_step), 'weights':[dist_power_11, dist_power_12, dist_power_13]}\n",
    "\n",
    "gscv = GridSearchCV(model, grid, cv=cv_settings, verbose=2)\n",
    "gscv.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc745d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gscv.best_params_)\n",
    "print(gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8b0c3",
   "metadata": {},
   "source": [
    "### RepeatedKFold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809d834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prev_k = gscv.best_params_['n_neighbors']\n",
    "# rep_gscv_step = 1\n",
    "# search_radii = int(gscv_step + np.ceil(gscv_step/4))\n",
    "\n",
    "# print(prev_k, prev_k-search_radii, prev_k+search_radii+rep_gscv_step, rep_gscv_step)\n",
    "\n",
    "# cv_settings = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=12) \n",
    "\n",
    "# model = KNeighborsClassifier()\n",
    "# grid = {'n_neighbors':np.arange(prev_k-search_radii, prev_k+search_radii+rep_gscv_step, rep_gscv_step), 'weights':[dist_power_10, dist_power_11, dist_power_12]}\n",
    "\n",
    "# rep_gscv = GridSearchCV(model, grid, cv=cv_settings, verbose=2)\n",
    "# rep_gscv.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e58374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rep_gscv.best_params_)\n",
    "# print(rep_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107e6cb",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pd.DataFrame(rep_gscv.predict(X_train_final))\n",
    "\n",
    "print(y_preds.value_counts())\n",
    "\n",
    "acc_score = accuracy_score(y_train, y_preds)*100\n",
    "precision = precision_score(y_train, y_preds)*100\n",
    "recall = recall_score(y_train, y_preds)*100\n",
    "conf_matrix = confusion_matrix(y_train, y_preds)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "FPR = fp/(fp + tn)\n",
    "FNR = fn/(fn + tp)\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy: {round(acc_score, 4)}\")\n",
    "print(f\"Precision: {round(precision, 4)}\\tRecall: {round(recall, 4)}\")\n",
    "print(f\"FPR: {round(FPR*100, 4)}\\t\\tFNR: {round(FNR*100, 4)}\\n\")\n",
    "    \n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_test = pd.DataFrame(rep_gscv.predict(X_test_final)).rename({0:'predicted'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e478fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = pd.concat([test_final[['id', 'host_id']], y_preds_test], axis=1)\n",
    "overlapping_hosts = train_final[train_final['host_id'].isin(test_final['host_id'])].drop_duplicates('host_id')[['host_id', 'host_is_superhost']]\n",
    "\n",
    "def overwrite(row):\n",
    "    if row['host_id'] in overlapping_hosts['host_id'].values:\n",
    "        row['predicted'] = overlapping_hosts[overlapping_hosts['host_id'] == row['host_id']]['host_is_superhost'].values[0]\n",
    "    return row\n",
    "        \n",
    "    \n",
    "predicted_values = predicted_values.apply(overwrite, axis=1)\n",
    "predicted_values = predicted_values[['id', 'predicted']].set_index('id')\n",
    "predicted_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## 4) Put any ad-hoc steps for further improving model accuracy\n",
    "For example, scaling up or scaling down the predictions, capping predictions, etc.\n",
    "\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07599c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## 5) Export the predictions in the format required to submit on Kaggle\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_values.to_csv('pred_csvs/KNN_class_model_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
